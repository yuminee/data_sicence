# -*- coding: utf-8 -*-
"""tweet_disaster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zgRQLmjDH6UmtN4EnqU9XyB6NI9VSKFr

# Real or Not? NLP with Disaster Tweets

### Importing required Libraries.

ê°€ì¥ ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ import í•©ë‹ˆë‹¤.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict
from collections import Counter
plt.style.use('ggplot')
stop=set(stopwords.words('english'))
import re
from nltk.tokenize import word_tokenize
import gensim
import string
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tqdm import tqdm
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from keras.initializers import Constant
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam

import nltk
nltk.download("stopwords")

import os

tweet = pd.read_csv('/content/drive/MyDrive/kaggle/tweetDisater/data/train.csv')
test = pd.read_csv('/content/drive/MyDrive/kaggle/tweetDisater/data/test.csv')
tweet.head(3)

x = tweet.target.value_counts()
sns.barplot(x.index, x)
plt.gca().set_ylabel('samples')

"""ê²°ê³¼ë¥¼ ë³´ë‹ˆê¹Œ class 0(No Disaster)ì´ class 1(disaster tweets)ë³´ë‹¤ ë” ë§ë„¤ìš”.

### Exploratory Data Analysis of tweets

#### Number of characters in **tweets**
"""

fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5)) #figsize = ê·¸ë¦¼ì˜ ê°€ë¡œ ì„¸ë¡œ ì¸ì¹˜ ë‹¨ìœ„
tweet_len = tweet[tweet['target'] == 1]['text'].str.len()
ax1.hist(tweet_len, color='red')
ax1.set_title('disater tweets')
tweet_len = tweet[tweet['target']==0]['text'].str.len()
ax2.hist(tweet_len, color='green')
ax2.set_title('Not disaster tweets')
fig.suptitle('Character in tweets')
plt.show()

"""#### Number of words in a tweet"""

fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5)) #ë‘ê°œ ê·¸ë˜í”„ ì„¤ì •ì„ í•œë²ˆì— subplots
tweet_len = tweet[tweet['target']==1]['text'].str.split().map(lambda x : len(x))
ax1.hist(tweet_len, color='red')
ax1.set_title('disater tweets')
tweet_len = tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))
ax2.hist(tweet_len,color='green')
ax1.set_title('Not disater tweets')
fig.suptitle('Words in a tweet')
plt.show()

"""#### Average word length in a tweet"""

fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5))
word=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x : np.mean(x)), ax = ax1, color='red') # ë§‰ëŒ€ ê·¸ë˜í”„ + meanê°’ìœ¼ë¡œ ì„ 
ax1.set_title('disaster')
word = tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x : np.mean(x)), ax = ax2, color = 'green')
ax2.set_title('Not disaster')
fig.suptitle('Average word lentgh in a tweet')

def create_corpus(target):
    corpus=[]
    
    for x in tweet[tweet['target']==target]['text'].str.split():
        for i in x:
            corpus.append(i)
    return corpus #ìì—°ì–¸ì–´ ì—°êµ¬ë¥¼ ìœ„í•´ íŠ¹ì •í•œ ëª©ì ì„ ê°€ì§€ê³  ì–¸ì–´ì˜ í‘œë³¸ì„ ì¶”ì¶œí•œ ì§‘í•©

"""#### Common stopwords in tweets
- stopword(ë¶ˆìš©ì–´)
  : ê°–ê³  ìˆëŠ” ë°ì´í„°ì—ì„œ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ í† í°ë§Œì„ ì„ ë³„í•˜ê¸° ìœ„í•´ì„œëŠ” í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ í† í°ì„ ì œê±°í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•¨. ì—¬ê¸°ì„œ í° ì˜ë¯¸ê°€ ì—†ë‹¤ë¼ëŠ” ê²ƒì€ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ë¶„ì„ì„ í•˜ëŠ” ê²ƒì— ìˆì–´ì„œëŠ” í° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì„ ë§í•¨.

class 0 ë¶€í„° ë¶„ì„
"""

corpus = create_corpus(0)

dic = defaultdict(int) # ë””í´íŠ¸ ê°’ì´ intì¸ ë”•ì…”ë„ˆë¦¬.
for word in corpus:
  if word in stop:
    dic[word]+=1
    

top = sorted(dic.items(), key=lambda x :x [1], reverse = True) [:10]

x,y = zip(*top)
plt.bar(x,y)

corpus = create_corpus(1)

dic = defaultdict(int) # ë””í´íŠ¸ ê°’ì´ intì¸ ë”•ì…”ë„ˆë¦¬.
for word in corpus:
  if word in stop:
    dic[word]+=1
    

top = sorted(dic.items(), key=lambda x :x [1], reverse = True) [:10]

x,y = zip(*top)
plt.bar(x,y)

"""### Analyzing punctuations"""

plt.figure(figsize=(10,5))
corpus = create_corpus(1)

dic = defaultdict(int)
import string
special = string.punctuation
for i in (corpus) : 
  if i in special : 
    dic[i]+=1

x,y = zip(*dic.items())
plt.bar(x,y)

plt.figure(figsize=(10,5))
corpus = create_corpus(0)

dic = defaultdict(int)
import string
special = string.punctuation
for i in (corpus) : 
  if i in special : 
    dic[i]+=1

x,y = zip(*dic.items())
plt.bar(x,y, color = 'green')

"""### Common words?"""

counter = Counter(corpus)
most = counter.most_common()
x = []
y = []
for word, count in most [:40]:
  if (word not in stop) :
    x.append(word)
    y.append(count)

sns.barplot(x=y, y=x)

"""### Ngram analysis

bigram (n=2)
"""

def get_top_tweet_bigrams(corpus, n = None):
  vec = CountVectorizer(ngram_range = (2,2)).fit(corpus)
  bag_of_words = vec.transform(corpus)
  sum_words = bag_of_words.sum(axis=0)
  words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
  words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)
  return words_freq[:n]

plt.figure(figsize=(10,5))
top_tweet_bigrams = get_top_tweet_bigrams(tweet['text'])[:10]
x,y = map(list, zip(*top_tweet_bigrams))
sns.barplot(x=y, y=x)

"""### Data Cleaning"""

df = pd.concat([tweet, test])
df.shape

"""#### Removing urls"""

example="New competition launched :https://www.kaggle.com/c/nlp-getting-started"

def remove_URL(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'',text)

remove_URL(example)

df['text'] = df['text'].apply(lambda x : remove_URL(x))

example = """<div>
<h1>Real or Fake</h1>
<p>Kaggle </p>
<a href="https://www.kaggle.com/c/nlp-getting-started">getting started</a>
</div>"""

def remove_html(text):
    html=re.compile(r'<.*?>')
    return html.sub(r'',text)
print(remove_html(example))

df['text']=df['text'].apply(lambda x : remove_html(x))

"""#### Removing Emojis"""

# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b

def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

remove_emoji("Omg another Earthquake ğŸ˜”ğŸ˜”")

df['text']=df['text'].apply(lambda x: remove_emoji(x))

"""#### Removing punctuations"""

def remove_punct(text):
    table=str.maketrans('','',string.punctuation)
    return text.translate(table)

example="I am a #king"
print(remove_punct(example))

df['text']=df['text'].apply(lambda x : remove_punct(x))

"""#### Spelling Correction"""

!pip install pyspellchecker

from spellchecker import SpellChecker

spell = SpellChecker()
def correct_spellings(text):
    corrected_text = []
    misspelled_words = spell.unknown(text.split())
    for word in text.split():
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)
        
text = "corect me plese"
correct_spellings(text)

df['text']=df['text'].apply(lambda x : correct_spellings(x))

"""### GloVe for Vectorization"""

def create_corpus (df) : 
    corpus =[]
    for tweet in tqdm(df['text']):
      words = [word.lower() for word in word_tokenize(tweet) if((wod.isalpha()==1) & (word not in stop))]
      corpus.append(words)
    retirn corpus

