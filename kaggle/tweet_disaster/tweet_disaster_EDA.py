# -*- coding: utf-8 -*-
"""tweet_disaster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zgRQLmjDH6UmtN4EnqU9XyB6NI9VSKFr

# Real or Not? NLP with Disaster Tweets

### Importing required Libraries.

가장 먼저 필요한 라이브러리들을 import 합니다.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict
from collections import Counter
plt.style.use('ggplot')
stop=set(stopwords.words('english'))
import re
from nltk.tokenize import word_tokenize
import gensim
import string
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tqdm import tqdm
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from keras.initializers import Constant
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam

import nltk
nltk.download("stopwords")

import os

tweet = pd.read_csv('/content/drive/MyDrive/kaggle/tweetDisater/data/train.csv')
test = pd.read_csv('/content/drive/MyDrive/kaggle/tweetDisater/data/test.csv')
tweet.head(3)

x = tweet.target.value_counts()
sns.barplot(x.index, x)
plt.gca().set_ylabel('samples')

"""결과를 보니까 class 0(No Disaster)이 class 1(disaster tweets)보다 더 많네요.

### Exploratory Data Analysis of tweets

#### Number of characters in **tweets**
"""

fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5)) #figsize = 그림의 가로 세로 인치 단위
tweet_len = tweet[tweet['target'] == 1]['text'].str.len()
ax1.hist(tweet_len, color='red')
ax1.set_title('disater tweets')
tweet_len = tweet[tweet['target']==0]['text'].str.len()
ax2.hist(tweet_len, color='green')
ax2.set_title('Not disaster tweets')
fig.suptitle('Character in tweets')
plt.show()

"""#### Number of words in a tweet"""

fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5)) #두개 그래프 설정을 한번에 subplots
tweet_len = tweet[tweet['target']==1]['text'].str.split().map(lambda x : len(x))
ax1.hist(tweet_len, color='red')
ax1.set_title('disater tweets')
tweet_len = tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))
ax2.hist(tweet_len,color='green')
ax1.set_title('Not disater tweets')
fig.suptitle('Words in a tweet')
plt.show()

"""#### Average word length in a tweet"""

fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5))
word=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x : np.mean(x)), ax = ax1, color='red') # 막대 그래프 + mean값으로 선
ax1.set_title('disaster')
word = tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x : np.mean(x)), ax = ax2, color = 'green')
ax2.set_title('Not disaster')
fig.suptitle('Average word lentgh in a tweet')

def create_corpus(target):
    corpus=[]
    
    for x in tweet[tweet['target']==target]['text'].str.split():
        for i in x:
            corpus.append(i)
    return corpus #자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합

"""#### Common stopwords in tweets
- stopword(불용어)
  : 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요함. 여기서 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말함.

class 0 부터 분석
"""

corpus = create_corpus(0)

dic = defaultdict(int) # 디폴트 값이 int인 딕셔너리.
for word in corpus:
  if word in stop:
    dic[word]+=1
    

top = sorted(dic.items(), key=lambda x :x [1], reverse = True) [:10]

x,y = zip(*top)
plt.bar(x,y)

corpus = create_corpus(1)

dic = defaultdict(int) # 디폴트 값이 int인 딕셔너리.
for word in corpus:
  if word in stop:
    dic[word]+=1
    

top = sorted(dic.items(), key=lambda x :x [1], reverse = True) [:10]

x,y = zip(*top)
plt.bar(x,y)

"""### Analyzing punctuations"""

plt.figure(figsize=(10,5))
corpus = create_corpus(1)

dic = defaultdict(int)
import string
special = string.punctuation
for i in (corpus) : 
  if i in special : 
    dic[i]+=1

x,y = zip(*dic.items())
plt.bar(x,y)

plt.figure(figsize=(10,5))
corpus = create_corpus(0)

dic = defaultdict(int)
import string
special = string.punctuation
for i in (corpus) : 
  if i in special : 
    dic[i]+=1

x,y = zip(*dic.items())
plt.bar(x,y, color = 'green')

"""### Common words?"""

counter = Counter(corpus)
most = counter.most_common()
x = []
y = []
for word, count in most [:40]:
  if (word not in stop) :
    x.append(word)
    y.append(count)

sns.barplot(x=y, y=x)

"""### Ngram analysis

bigram (n=2)
"""

def get_top_tweet_bigrams(corpus, n = None):
  vec = CountVectorizer(ngram_range = (2,2)).fit(corpus)
  bag_of_words = vec.transform(corpus)
  sum_words = bag_of_words.sum(axis=0)
  words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
  words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)
  return words_freq[:n]

plt.figure(figsize=(10,5))
top_tweet_bigrams = get_top_tweet_bigrams(tweet['text'])[:10]
x,y = map(list, zip(*top_tweet_bigrams))
sns.barplot(x=y, y=x)

"""### Data Cleaning"""

df = pd.concat([tweet, test])
df.shape

"""#### Removing urls"""

example="New competition launched :https://www.kaggle.com/c/nlp-getting-started"

def remove_URL(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'',text)

remove_URL(example)

df['text'] = df['text'].apply(lambda x : remove_URL(x))

example = """<div>
<h1>Real or Fake</h1>
<p>Kaggle </p>
<a href="https://www.kaggle.com/c/nlp-getting-started">getting started</a>
</div>"""

def remove_html(text):
    html=re.compile(r'<.*?>')
    return html.sub(r'',text)
print(remove_html(example))

df['text']=df['text'].apply(lambda x : remove_html(x))

"""#### Removing Emojis"""

# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b

def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

remove_emoji("Omg another Earthquake 😔😔")

df['text']=df['text'].apply(lambda x: remove_emoji(x))

"""#### Removing punctuations"""

def remove_punct(text):
    table=str.maketrans('','',string.punctuation)
    return text.translate(table)

example="I am a #king"
print(remove_punct(example))

df['text']=df['text'].apply(lambda x : remove_punct(x))

"""#### Spelling Correction"""

!pip install pyspellchecker

from spellchecker import SpellChecker

spell = SpellChecker()
def correct_spellings(text):
    corrected_text = []
    misspelled_words = spell.unknown(text.split())
    for word in text.split():
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)
        
text = "corect me plese"
correct_spellings(text)

df['text']=df['text'].apply(lambda x : correct_spellings(x))

"""### GloVe for Vectorization"""

def create_corpus (df) : 
    corpus =[]
    for tweet in tqdm(df['text']):
      words = [word.lower() for word in word_tokenize(tweet) if((wod.isalpha()==1) & (word not in stop))]
      corpus.append(words)
    retirn corpus

